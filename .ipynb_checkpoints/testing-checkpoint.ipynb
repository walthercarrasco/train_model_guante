{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6cbce5-fb2b-40e0-9e61-8ab690d1b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 15:26:13.286117: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 15:26:13.525907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748121973.610949    1596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748121973.633445    1596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748121973.799888    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748121973.799931    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748121973.799932    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748121973.799933    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 15:26:13.818895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f39228-2fb0-4360-b466-858268f351b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748122328.117254    1596 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class FeatureWeightingLayer(Layer):\n",
    "    def __init__(self, scaling_factors, **kwargs):\n",
    "        super(FeatureWeightingLayer, self).__init__(**kwargs)\n",
    "        self.scaling_factors = K.variable(scaling_factors, name='scaling_factors')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scaling_factors\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(FeatureWeightingLayer, self).get_config()\n",
    "        config.update({\"scaling_factors\": self.scaling_factors.numpy()})\n",
    "        return config\n",
    "\n",
    "# Load model directly with custom_objects parameter\n",
    "model = load_model('./model.h5', custom_objects={'FeatureWeightingLayer': FeatureWeightingLayer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af2123a-1533-4e28-b26f-bdee75df7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESANDO DATOS DE SENSOR ===\n",
      "\n",
      "1. Limpiando datos...\n",
      "Líneas originales: 389\n",
      "Líneas válidas después de limpieza: 386\n",
      "DataFrame creado con forma: (386, 11)\n",
      "\n",
      "Primeras 5 filas:\n",
      "   angle1  angle2  angle3  angle4  angle5  rolldeg  pitchdeg  anglegx  \\\n",
      "0   27.00   27.00   27.00   27.00   10.20    -6.83     -90.0    -0.02   \n",
      "1   45.90   45.90   45.90   45.90   19.74   -14.16     -90.0    -0.03   \n",
      "2   59.13   59.13   59.13   59.13   27.62    17.43     -90.0     0.09   \n",
      "3   68.39   68.39   68.39   68.39   33.13    16.88     -90.0     0.41   \n",
      "4   74.87   74.87   74.87   74.87   36.99    21.30     -90.0     0.50   \n",
      "\n",
      "   anglegy  anglegz  timestamp  \n",
      "0     0.09    -0.07     10.244  \n",
      "1     1.15    -1.20     10.296  \n",
      "2     2.07    -2.67     10.347  \n",
      "3     3.06    -4.25     10.399  \n",
      "4     3.86    -5.68     10.450  \n",
      "\n",
      "Últimas 5 filas:\n",
      "     angle1  angle2  angle3  angle4  angle5  rolldeg  pitchdeg  anglegx  \\\n",
      "381     0.0   71.04   59.47    6.28    2.10   -64.25    -16.94    43.27   \n",
      "382     0.0   76.73   68.63    4.40    9.27   -44.88    -28.50    50.43   \n",
      "383     0.0   80.71   75.04    3.08   18.19   -61.55    -13.39    58.04   \n",
      "384     0.0   83.50   79.53    2.15   34.93   -53.89    -16.26    65.07   \n",
      "385     0.0   85.45   82.67    1.51   48.45   -57.22    -14.39    70.83   \n",
      "\n",
      "     anglegy  anglegz  timestamp  \n",
      "381   -63.21   -34.64     29.765  \n",
      "382   -65.08   -34.01     29.816  \n",
      "383   -69.04   -34.17     29.867  \n",
      "384   -72.82   -34.46     29.919  \n",
      "385   -77.79   -34.88     29.969  \n",
      "\n",
      "2. Cargando modelo...\n",
      "\n",
      "3. Haciendo predicciones...\n",
      "Datos de sensores: (386, 10)\n",
      "Usando intervalos fijos...\n",
      "\n",
      "4. Analizando resultados...\n",
      "\n",
      "=== ANÁLISIS DE PREDICCIONES ===\n",
      "Total de predicciones: 6\n",
      "Predicciones con confianza >= 0.7: 4\n",
      "\n",
      "Distribución de predicciones (confianza >= 0.7):\n",
      "  H: 2 predicciones\n",
      "  A: 1 predicciones\n",
      "  O: 1 predicciones\n",
      "\n",
      "Ejemplo de predicciones con alta confianza:\n",
      "  13.244s - 16.244s: A (confianza: 0.974)\n",
      "  16.244s - 19.244s: H (confianza: 0.918)\n",
      "  22.244s - 25.244s: O (confianza: 0.964)\n",
      "  25.244s - 28.244s: H (confianza: 0.946)\n",
      "\n",
      "Predicciones guardadas en: ./sensor_data_20250524_144255_predictions.csv\n",
      "Tiempo 13.2-16.2s: A (0.974)\n",
      "Tiempo 16.2-19.2s: H (0.918)\n",
      "Tiempo 22.2-25.2s: O (0.964)\n",
      "Tiempo 25.2-28.2s: H (0.946)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import re\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class FeatureWeightingLayer(Layer):\n",
    "    def __init__(self, scaling_factors, **kwargs):\n",
    "        super(FeatureWeightingLayer, self).__init__(**kwargs)\n",
    "        self.scaling_factors = K.variable(scaling_factors, name='scaling_factors')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scaling_factors\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(FeatureWeightingLayer, self).get_config()\n",
    "        config.update({\"scaling_factors\": self.scaling_factors.numpy()})\n",
    "        return config\n",
    "\n",
    "def clean_sensor_data(csv_path):\n",
    "    \"\"\"\n",
    "    Limpia los datos del CSV eliminando líneas no numéricas y procesando correctamente\n",
    "    \n",
    "    Args:\n",
    "        csv_path: ruta al archivo CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer todas las líneas del archivo\n",
    "    with open(csv_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Filtrar solo las líneas que contienen datos numéricos\n",
    "    clean_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Saltar líneas vacías\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Saltar la línea de encabezado\n",
    "        if line.startswith('angle1,angle2'):\n",
    "            continue\n",
    "            \n",
    "        # Saltar líneas que contienen texto informativo\n",
    "        if any(text in line for text in ['Iniciando', 'Smart Glove', 'Formato:', 'Flex1']):\n",
    "            continue\n",
    "        \n",
    "        # Verificar si la línea contiene datos numéricos válidos\n",
    "        # Debe tener el formato: num,num,num,num,num,num,num,num,num,num,timestamp\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        if len(parts) >= 11:  # Al menos 10 valores + timestamp\n",
    "            try:\n",
    "                # Intentar convertir los primeros 10 valores a float\n",
    "                values = [float(x) for x in parts[:10]]\n",
    "                # Si llegamos aquí, la línea es válida\n",
    "                clean_lines.append(line)\n",
    "            except ValueError:\n",
    "                # Si no se puede convertir, saltar esta línea\n",
    "                continue\n",
    "    \n",
    "    print(f\"Líneas originales: {len(lines)}\")\n",
    "    print(f\"Líneas válidas después de limpieza: {len(clean_lines)}\")\n",
    "    \n",
    "    # Crear DataFrame con las líneas limpias\n",
    "    if not clean_lines:\n",
    "        raise ValueError(\"No se encontraron datos válidos en el archivo\")\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    data_rows = []\n",
    "    for line in clean_lines:\n",
    "        parts = line.split(',')\n",
    "        # Tomar solo los primeros 10 valores (sensores) + timestamp\n",
    "        row = [float(x) for x in parts[:10]] + [float(parts[10])]\n",
    "        data_rows.append(row)\n",
    "    \n",
    "    column_names = ['angle1', 'angle2', 'angle3', 'angle4', 'angle5', \n",
    "                   'rolldeg', 'pitchdeg', 'anglegx', 'anglegy', 'anglegz', 'timestamp']\n",
    "    \n",
    "    df = pd.DataFrame(data_rows, columns=column_names)\n",
    "    \n",
    "    print(f\"DataFrame creado con forma: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nÚltimas 5 filas:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_continuous_data(model, df, label_map, strategy='sliding_window', window_size=68, step_size=10):\n",
    "    \"\"\"\n",
    "    Predice sobre datos continuos sin marcadores Start/End\n",
    "    \n",
    "    Args:\n",
    "        model: modelo LSTM entrenado\n",
    "        df: DataFrame con datos limpios\n",
    "        label_map: mapeo de etiquetas\n",
    "        strategy: estrategia de predicción\n",
    "        window_size: tamaño de ventana (68)\n",
    "        step_size: paso entre ventanas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer solo las columnas de sensores (sin timestamp)\n",
    "    sensor_data = df[['angle1', 'angle2', 'angle3', 'angle4', 'angle5', \n",
    "                     'rolldeg', 'pitchdeg', 'anglegx', 'anglegy', 'anglegz']].values\n",
    "    \n",
    "    timestamps = df['timestamp'].values\n",
    "    \n",
    "    print(f\"Datos de sensores: {sensor_data.shape}\")\n",
    "    \n",
    "    # Crear mapeo inverso\n",
    "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    if strategy == 'sliding_window':\n",
    "        print(\"Usando ventana deslizante...\")\n",
    "        \n",
    "        for i in range(0, len(sensor_data) - window_size + 1, step_size):\n",
    "            # Extraer ventana\n",
    "            window = sensor_data[i:i + window_size]\n",
    "            window_timestamps = timestamps[i:i + window_size]\n",
    "            \n",
    "            # Agregar dimensión de batch\n",
    "            window_batch = np.expand_dims(window, axis=0)\n",
    "            \n",
    "            # Predecir\n",
    "            prediction = model.predict(window_batch, verbose=0)\n",
    "            predicted_class = np.argmax(prediction[0])\n",
    "            confidence = np.max(prediction[0])\n",
    "            predicted_label = reverse_label_map[predicted_class]\n",
    "            \n",
    "            predictions.append({\n",
    "                'start_frame': i,\n",
    "                'end_frame': i + window_size - 1,\n",
    "                'start_time': window_timestamps[0],\n",
    "                'end_time': window_timestamps[-1],\n",
    "                'duration_seconds': window_timestamps[-1] - window_timestamps[0],\n",
    "                'predicted_label': predicted_label,\n",
    "                'confidence': confidence,\n",
    "                'probabilities': prediction[0].tolist()\n",
    "            })\n",
    "    \n",
    "    elif strategy == 'fixed_intervals':\n",
    "        print(\"Usando intervalos fijos...\")\n",
    "        \n",
    "        # Calcular duración total\n",
    "        total_duration = timestamps[-1] - timestamps[0]\n",
    "        interval_duration = 3.0  # 2 segundos por intervalo\n",
    "        \n",
    "        current_time = timestamps[0]\n",
    "        \n",
    "        while current_time + interval_duration <= timestamps[-1]:\n",
    "            # Encontrar índices para este intervalo de tiempo\n",
    "            start_idx = np.argmax(timestamps >= current_time)\n",
    "            end_time = current_time + interval_duration\n",
    "            end_idx = np.argmax(timestamps >= end_time)\n",
    "            \n",
    "            if end_idx == 0:  # Si no encontramos el final, usar el último índice\n",
    "                end_idx = len(timestamps) - 1\n",
    "            \n",
    "            # Extraer datos para este intervalo\n",
    "            interval_data = sensor_data[start_idx:end_idx]\n",
    "            \n",
    "            if len(interval_data) >= 10:  # Asegurar que tenemos suficientes datos\n",
    "                # Redimensionar a window_size muestras\n",
    "                if len(interval_data) >= window_size:\n",
    "                    indices = np.random.choice(len(interval_data), size=window_size, replace=False)\n",
    "                else:\n",
    "                    indices = np.random.choice(len(interval_data), size=window_size, replace=True)\n",
    "                \n",
    "                indices = np.sort(indices)\n",
    "                sequence = interval_data[indices]\n",
    "                \n",
    "                # Predecir\n",
    "                sequence_batch = np.expand_dims(sequence, axis=0)\n",
    "                prediction = model.predict(sequence_batch, verbose=0)\n",
    "                predicted_class = np.argmax(prediction[0])\n",
    "                confidence = np.max(prediction[0])\n",
    "                predicted_label = reverse_label_map[predicted_class]\n",
    "                \n",
    "                predictions.append({\n",
    "                    'start_frame': start_idx,\n",
    "                    'end_frame': end_idx - 1,\n",
    "                    'start_time': current_time,\n",
    "                    'end_time': end_time,\n",
    "                    'duration_seconds': interval_duration,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'confidence': confidence,\n",
    "                    'probabilities': prediction[0].tolist()\n",
    "                })\n",
    "            \n",
    "            current_time += interval_duration\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def analyze_predictions(predictions, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Analiza las predicciones y proporciona un resumen\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== ANÁLISIS DE PREDICCIONES ===\")\n",
    "    print(f\"Total de predicciones: {len(predictions)}\")\n",
    "    \n",
    "    # Filtrar predicciones con alta confianza\n",
    "    high_confidence_preds = [p for p in predictions if p['confidence'] >= confidence_threshold]\n",
    "    print(f\"Predicciones con confianza >= {confidence_threshold}: {len(high_confidence_preds)}\")\n",
    "    \n",
    "    # Contar predicciones por etiqueta\n",
    "    label_counts = {}\n",
    "    for pred in high_confidence_preds:\n",
    "        label = pred['predicted_label']\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "    \n",
    "    print(f\"\\nDistribución de predicciones (confianza >= {confidence_threshold}):\")\n",
    "    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {count} predicciones\")\n",
    "    \n",
    "    # Mostrar algunas predicciones de ejemplo\n",
    "    print(f\"\\nEjemplo de predicciones con alta confianza:\")\n",
    "    for i, pred in enumerate(high_confidence_preds[:10]):  # Primeras 10\n",
    "        print(f\"  {pred['start_time']:.3f}s - {pred['end_time']:.3f}s: {pred['predicted_label']} (confianza: {pred['confidence']:.3f})\")\n",
    "    \n",
    "    return high_confidence_preds\n",
    "\n",
    "def save_predictions_to_csv(predictions, output_path):\n",
    "    \"\"\"\n",
    "    Guarda las predicciones en un archivo CSV\n",
    "    \"\"\"\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nPredicciones guardadas en: {output_path}\")\n",
    "\n",
    "# FUNCIÓN PRINCIPAL\n",
    "def process_and_predict(csv_path, model_path, label_map, strategy='sliding_window'):\n",
    "    \"\"\"\n",
    "    Función principal que procesa todo el pipeline\n",
    "    \"\"\"\n",
    "    print(\"=== PROCESANDO DATOS DE SENSOR ===\")\n",
    "    \n",
    "    # 1. Limpiar datos\n",
    "    print(\"\\n1. Limpiando datos...\")\n",
    "    df = clean_sensor_data(csv_path)\n",
    "    \n",
    "    # 2. Cargar modelo\n",
    "    print(\"\\n2. Cargando modelo...\")\n",
    "    model = load_model('./model.h5', custom_objects={'FeatureWeightingLayer': FeatureWeightingLayer})\n",
    "    \n",
    "    # 3. Hacer predicciones\n",
    "    print(\"\\n3. Haciendo predicciones...\")\n",
    "    predictions = predict_continuous_data(model, df, label_map, strategy)\n",
    "    \n",
    "    # 4. Analizar resultados\n",
    "    print(\"\\n4. Analizando resultados...\")\n",
    "    high_conf_preds = analyze_predictions(predictions)\n",
    "    \n",
    "    # 5. Guardar resultados\n",
    "    output_path = csv_path.replace('.csv', '_predictions.csv')\n",
    "    save_predictions_to_csv(predictions, output_path)\n",
    "    \n",
    "    return predictions, high_conf_preds\n",
    "\n",
    "# EJEMPLO DE USO\n",
    "\"\"\"\n",
    "# Tu label_map del entrenamiento (debes tenerlo guardado)\n",
    "label_map = {'A': 0, 'L': 1, 'H': 2, 'O': 3}\n",
    "\n",
    "# Procesar y predecir\n",
    "predictions, high_conf = process_and_predict(\n",
    "    csv_path='./sensor_data_20250524_144255.csv',\n",
    "    model_path='./model.h5',\n",
    "    label_map=label_map,\n",
    "    strategy='sliding_window'  # o 'fixed_intervals'\n",
    ")\n",
    "\n",
    "# Ver predicciones más confiables\n",
    "for pred in high_conf[:5]:\n",
    "    print(f\"Tiempo {pred['start_time']:.1f}-{pred['end_time']:.1f}s: {pred['predicted_label']} ({pred['confidence']:.3f})\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "label_map = {'A': 0, 'L': 1, 'H': 2, 'O': 3}\n",
    "\n",
    "# Procesar y predecir\n",
    "predictions, high_conf = process_and_predict(\n",
    "    csv_path='./sensor_data_20250524_144255.csv',\n",
    "    model_path='./model.h5',\n",
    "    label_map=label_map,\n",
    "    strategy='fixed_intervals'\n",
    ")\n",
    "\n",
    "# Ver predicciones más confiables\n",
    "for pred in high_conf[:5]:\n",
    "    print(f\"Tiempo {pred['start_time']:.1f}-{pred['end_time']:.1f}s: {pred['predicted_label']} ({pred['confidence']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf60eb-6c6a-4542-b519-2b4cbabe529f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
